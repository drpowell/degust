library(limma)
library(edgeR)
library(jsonlite)

#From _common
count_cols <- <%== @columns %>
data<-read.delim('<%== @counts_file %>', sep="<%== @sep_char %>", check.names=FALSE, colClasses='character', na.strings=c())

rzr_col <- data$`Peptide counts (razor+unique)`
rzr_uniq <- lapply(lapply(mapply(rzr_col, FUN = strsplit, ";"), FUN = as.numeric), FUN = sum)


data <- data[data$Reverse != "+" & data$`Only identified by site` != "+" & rzr_uniq > 1,]
data[,count_cols] <- apply(data[,count_cols], 2, function(v) as.numeric(v)) # Force numeric count columns
data <- data[data$`Potential contaminant` == "",] # Filter rows with potential contaminants
data <- data[data$Reverse == "",] # Filter rows in reverse direction

counts <- as.matrix(data[, count_cols])
#Remove rows with >= 20% missing values (Missing mean NA, not 0)
keepNonZero <- rowMeans(counts != 0) >= <%= @min_cpm %>
keepIntensity <- rowSums(cpm(counts)> <%= @min_counts %>) >= <%= @min_cpm_samples %>

keep <- keepNonZero & keepIntensity
data <- data[keep,]
counts <- counts[keep,]
#Variance Stabilise

#Imputation starts here.
#lowpt <- median(as.matrix(counts)) - (sd(as.matrix(counts)) * 1.5)
#highpt <- median(as.matrix(counts)) - (sd(as.matrix(counts)) * 2.1)
#mid_vals <- counts[counts < highpt & lowpt < counts]
#Select values at random which are <= 1.8 SD from the median and fill them into NA's?
#counts[is.na(counts)] <- mid_vals[floor(runif(n=sum(is.na(counts)), min = 0, max = length(mid_vals)))]
#Select values at random from normal distribution which take values from 1.5-2.1SD from median.
#How do we produce a normal distribution that can select from it?

#Instead of that, we follow what DEP does

#We set 0 to NA so we can easily replace them
counts[counts == 0] <- NA

#Impute on log2 counts
counts <-  log2(counts)

stat_tb <- data.frame(matrix(nrow = ncol(counts), ncol=4))
for(i in seq_len(ncol(counts))){
    stat_tb[i,] <- c(colnames(counts)[i], median(counts[,i], na.rm = T), sd(counts[,i], na.rm = T), sum(is.na(counts[,i])))
}
stat_tb[,2:4] <- apply(X = stat_tb[,2:4], FUN=as.numeric, MARGIN = 2)
colnames(stat_tb) <- c("samples", "median", "sd", "missing")

#SHIFT & SCALE
shift = 1.8
scale = 0.3

for (a in seq_len(nrow(stat_tb))) {
    counts[is.na(counts[, stat_tb$samples[a]]), stat_tb$samples[a]] <- rnorm(
        n = stat_tb$missing[a],
        mean = stat_tb$median[a] - shift * stat_tb$sd[a],
        sd = stat_tb$sd[a] * scale)
    }
colnames(counts) <- paste(colnames(counts), "imputed")

#No normalisaiton present?
design <- <%== @design %>
cont.matrix <- <%== @cont_matrix %>

fit <- lmFit(counts,design)
fit2 <- contrasts.fit(fit, cont.matrix)
fit2 <- eBayes(fit2)

out <- topTable(fit2, n=Inf, sort.by='none')

out2 <- cbind(fit2$coef,
              out[, c('P.Value','adj.P.Val','AveExpr')],
              data[, c(<%== @export_cols %>)],
              counts)

param_normalized <- '<%= @normalized %>'
normalized <- matrix(0)
if (param_normalized=='') {
    write.csv(out2, file="<%== @output_dir %>/output.txt", row.names=FALSE,na='')
} else if (param_normalized=='backend') {
    normalized <- out2
} else if (param_normalized=='remove-hidden') {
    hidden_factors <- <%== @hidden_factors %>
    if (length(hidden_factors)>0) {
        # Remove the batch effect (as done in removeBatchEffect)
        beta <- fit$coefficients[, hidden_factors, drop=FALSE]
        normalized <- as.matrix(y) - beta %*% t(design[, hidden_factors, drop=FALSE])
    }
}

cat(
   toJSON(list(rank=fit2$rank, df_prior=fit2$df.prior,
               design=data.frame(fit2$design), contrasts=data.frame(fit2$contrasts),
               cov_coefficients=data.frame(fit2$cov.coefficients),
               normalized=list(columns=colnames(normalized), values=normalized)
         )),
   file="<%== @output_dir %>/extra.json"
)
